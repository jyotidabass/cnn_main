{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_main.ipynb",
      "provenance": [],
      "mount_file_id": "19lfQUAvFmu2ErV7wcniuy83hTEenfxhh",
      "authorship_tag": "ABX9TyOwSDhdMp5IjB/3OzEIA63V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyotidabass/cnn_main/blob/main/cnn_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GLD-AKLoKFD",
        "outputId": "1ed22126-a74d-4844-af3d-ce9b251d937a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved\n",
            "Epoch 1, Train Accuracy: 0.2357378602027893 , TrainLoss: 0.005053614266216755 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 2, Train Accuracy: 0.4479019343852997 , TrainLoss: 0.004450668580830097 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 3, Train Accuracy: 0.5186232924461365 , TrainLoss: 0.004146424122154713 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 4, Train Accuracy: 0.5893446207046509 , TrainLoss: 0.00392500264570117 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 5, Train Accuracy: 0.6364921927452087 , TrainLoss: 0.0034992697183042765 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 6, Train Accuracy: 0.6600660085678101 , TrainLoss: 0.0031790274661034346 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 7, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.002877089660614729 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 8, Train Accuracy: 0.6600660085678101 , TrainLoss: 0.0026802346110343933 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 9, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.002355852397158742 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 10, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.002192991552874446 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 11, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.001964317634701729 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 12, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.0018465094035491347 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 13, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.0016944666858762503 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 14, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.0015642635989934206 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 15, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.001467255293391645 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 16, Train Accuracy: 0.6600660085678101 , TrainLoss: 0.0014433955075219274 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 17, Train Accuracy: 0.6600660085678101 , TrainLoss: 0.001361674047075212 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 18, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.0012660137144848704 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 19, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.0012091434327885509 , Test Accuracy: 0.3771805763244629\n",
            "Epoch 20, Train Accuracy: 0.6836398243904114 , TrainLoss: 0.0011683296179398894 , Test Accuracy: 0.3771805763244629\n"
          ]
        }
      ],
      "source": [
        "from multiprocessing import freeze_support\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, Sampler\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Hyperparameters.\n",
        "num_epochs = 20\n",
        "num_classes = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "num_of_workers = 5\n",
        "\n",
        "DATA_PATH_TRAIN = Path('/content/drive/MyDrive/Tongue/Train')\n",
        "DATA_PATH_TEST = Path('/content/drive/MyDrive/Tongue_256/Validation_256')\n",
        "MODEL_STORE_PATH = Path('/content/drive/MyDrive')\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "# Flowers dataset.\n",
        "train_dataset = datasets.ImageFolder(root=DATA_PATH_TRAIN, transform=trans)\n",
        "test_dataset = datasets.ImageFolder(root=DATA_PATH_TEST, transform=trans)\n",
        "\n",
        "# Create custom random sampler class to iter over dataloader.\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_of_workers)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_of_workers)\n",
        "\n",
        "# CNN we are going to implement.\n",
        "class Unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Unit, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, kernel_size=3, out_channels=out_channels, stride=1, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        output = self.bn(output)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CNNet(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(CNNet, self).__init__()\n",
        "\n",
        "        # Create 14 layers of the unit with max pooling in between\n",
        "        self.unit1 = Unit(in_channels=3, out_channels=32)\n",
        "        self.unit2 = Unit(in_channels=32, out_channels=32)\n",
        "        self.unit3 = Unit(in_channels=32, out_channels=32)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.unit4 = Unit(in_channels=32, out_channels=64)\n",
        "        self.unit5 = Unit(in_channels=64, out_channels=64)\n",
        "        self.unit6 = Unit(in_channels=64, out_channels=64)\n",
        "        self.unit7 = Unit(in_channels=64, out_channels=64)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.unit8 = Unit(in_channels=64, out_channels=128)\n",
        "        self.unit9 = Unit(in_channels=128, out_channels=128)\n",
        "        self.unit10 = Unit(in_channels=128, out_channels=128)\n",
        "        self.unit11 = Unit(in_channels=128, out_channels=128)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.unit12 = Unit(in_channels=128, out_channels=128)\n",
        "        self.unit13 = Unit(in_channels=128, out_channels=128)\n",
        "        self.unit14 = Unit(in_channels=128, out_channels=128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=4)\n",
        "\n",
        "        # Add all the units into the Sequential layer in exact order\n",
        "        self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, self.unit4, self.unit5, self.unit6\n",
        "                                 , self.unit7, self.pool2, self.unit8, self.unit9, self.unit10, self.unit11, self.pool3,\n",
        "                                 self.unit12, self.unit13, self.unit14, self.avgpool)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=128, out_features=num_class)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.net(input)\n",
        "        output = output.view(-1, 128)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Check if gpu support is available\n",
        "cuda_avail = torch.cuda.is_available()\n",
        "\n",
        "# Create model, optimizer and loss function\n",
        "model = CNNet(num_classes)\n",
        "\n",
        "# if cuda is available, move the model to the GPU\n",
        "if cuda_avail:\n",
        "    model.cuda()\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def save_models(epoch):\n",
        "    torch.save(model.state_dict(), f\"{epoch}.model\")\n",
        "    print(\"Checkpoint saved\")\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_acc = 0.0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "\n",
        "        if cuda_avail:\n",
        "            images = Variable(images.cuda())\n",
        "            labels = Variable(labels.cuda())\n",
        "\n",
        "        # Predict classes using images from the test set\n",
        "        outputs = model(images)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "        test_acc += torch.sum(prediction == labels.data).float()\n",
        "\n",
        "    # Compute the average acc and loss over all 10000 test images\n",
        "    test_acc = test_acc / 4242 * 100\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "def train(num_epoch):\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        model.train()\n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            # Move images and labels to gpu if available\n",
        "            if cuda_avail:\n",
        "                images = Variable(images.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "\n",
        "            # Clear all accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predict classes using images from the test set\n",
        "            outputs = model(images)\n",
        "            # Compute the loss based on the predictions and actual labels\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Adjust parameters according to the computed gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.cpu().data * images.size(0)\n",
        "            _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "            train_acc += torch.sum(prediction == labels.data).float()\n",
        "\n",
        "        # Call the learning rate adjustment function\n",
        "        #adjust_learning_rate(epoch)\n",
        "\n",
        "        # Compute the average acc and loss over all 50000 training images\n",
        "        train_acc = train_acc / 4242 * 100\n",
        "        train_loss = train_loss / 8484\n",
        "\n",
        "        # Evaluate on the test set\n",
        "        test_acc = test()\n",
        "\n",
        "        # Save the model if the test acc is greater than our current best\n",
        "        if test_acc > best_acc:\n",
        "            save_models(epoch)\n",
        "            best_acc = test_acc\n",
        "\n",
        "        # Print the metrics\n",
        "        print(f\"Epoch {epoch + 1}, Train Accuracy: {train_acc} , TrainLoss: {train_loss} , Test Accuracy: {test_acc}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    freeze_support()\n",
        "    train(num_epochs)"
      ]
    }
  ]
}